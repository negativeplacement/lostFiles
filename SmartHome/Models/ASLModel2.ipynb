{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "# Data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Visualization\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers, Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Suppressing warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(static_image_mode=True,\n",
    "                      max_num_hands=1,\n",
    "                      min_detection_confidence=0.75,\n",
    "                      min_tracking_confidence=0.75,\n",
    "                      model_complexity=0)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "dataset_path = 'asl_alphabet_train/ASLMODEL'  # Update this path to your dataset location\n",
    "\n",
    "def process_and_display_image(image_path):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    \n",
    "    # Process the image\n",
    "    results = hands.process(img)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        # Draw hand landmarks\n",
    "        h, w, c = img.shape\n",
    "        min_x, min_y = w, h\n",
    "        max_x, max_y = 0, 0\n",
    "        for id, lm in enumerate(results.multi_hand_landmarks[0].landmark):\n",
    "            cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "            min_x, min_y = min(min_x, cx), min(min_y, cy)\n",
    "            max_x, max_y = max(max_x, cx), max(max_y, cy)\n",
    "        \n",
    "        center_x, center_y = (min_x + max_x) // 2, (min_y + max_y) // 2\n",
    "        width, height = 1.3*(max_x - min_x), 1.3*(max_y - min_y)\n",
    "        \n",
    "        new_min_x, new_min_y = int(center_x - width / 2), int(center_y -  height / 2)\n",
    "        new_max_x, new_max_y = int(center_x + width / 2), int(center_y +  height / 2)\n",
    "        \n",
    "        cv2.rectangle(img, (new_min_x, new_min_y), (new_max_x, new_max_y), (255, 255, 25), 1)\n",
    "        hand_region = img[new_min_y:new_max_y, new_min_x:new_max_x]\n",
    "        \n",
    "        filename= '/ASLMODELCROPPED/'+ image_path.split('/')[-1]\n",
    "        print(filename)\n",
    "        cv2.imwrite(filename, hand_region)\n",
    "        \n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No hand detected in:', image_path)\n",
    "\n",
    "\n",
    "for class_dir in os.listdir('asl_alphabet_train/ASLMODEL')[::-1]:\n",
    "    class_path = os.path.join('asl_alphabet_train/ASLMODEL', class_dir)\n",
    "    if os.path.isdir(class_path):\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            process_and_display_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ImageDataGenerator without augmentation, only rescaling\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale pixel values from [0, 255] to [0, 1] for neural network processing\n",
    "    validation_split=0.2  # Assuming you want to split your dataset into training and validation\n",
    ")\n",
    "\n",
    "# Training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'asl_alphabet_train/ASLMODEL',  # Replace with your actual directory path\n",
    "    target_size=(64, 64),  # Resize images to 200x200\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Since you're doing multi-class classification\n",
    "    subset='training'  # Specify as training data\n",
    ")\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'asl_alphabet_train/ASLMODEL',  # Same directory, ImageDataGenerator will split the data\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Specify as validation data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Conv2D(64, (5, 5), activation='relu', padding='same', input_shape=(64, 64, 3)),  # Adjusted for RGB images\n",
    "    layers.Conv2D(64, (5, 5), activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(2),\n",
    "    \n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(2),\n",
    "    \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    \n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(9, activation='softmax')  # Adjusted for 14 classes\n",
    "])\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "    min_delta = 1e-4,\n",
    "    patience = 5,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "# Adding a learning rate annealer\n",
    "reduceLR = ReduceLROnPlateau(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 3,\n",
    "    factor = 0.5,\n",
    "    min_lr = 1e-5\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[earlyStopping, reduceLR],\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    workers=4,\n",
    "    use_multiprocessing=False,\n",
    "    epochs=10  # You can adjust the number of epochs based on your model's performance and training needs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlternateModel = Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(640, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    \n",
    "    layers.Dense(9, activation='softmax')  # Adjusted for 14 classes\n",
    "])\n",
    "AlternateModel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Summary of model\n",
    "AlternateModel.summary()\n",
    "\n",
    "AlternateHistory = AlternateModel.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[earlyStopping, reduceLR],\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    workers=4,\n",
    "    use_multiprocessing=False,\n",
    "    epochs=10  # You can adjust the number of epochs based on your model's performance and training needs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AlternateModel.save('ASLModelV3.h5')\n",
    "AlternateModel.classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AltModel=load_model('ASLModelV3.h5')\n",
    "plot_model(AltModel, show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
